---
title: "In-Class Exercise 2 - Spatial Weights - sfdep"
author: "Dabbie Neo"
date: "19 Novemember 2023"
date-modified: "`r Sys.Date()`"
---

# 1. Overview

In this in-class exercise, an alternative R package to spdep package that will be used in . The package is called sfdep. Acc

# 2. Getting Started

## 2.1 Installing and Loading the R packages

Five R packages will used for this in-class exercise, they are **sf, tidyverse, sfdep, tmap** and **knitr**.

\- \*\*sf\*\*: for importing and handling of geospatial data

\- \*\*tidyverse\*\*: for performing data science tasks such as importing, wrangling and visualising data

\- \*\*sfdep\*\*: used to compute spatial weights, global and local spatial autocorrelation statistics, and

\- \*\*tmap\*\*: used to prepare cartographic quality choropleth map.

\- \*\*knitr\*\*: a general-purpose literate programming engine, with lightweight API's designed to give users full control of the output without heavy coding work.

```{r}
pacman::p_load(sf, sfdep, tmap, tidyverse, knitr, plotly)
```

## 2.2 The Data

Two data sets will be used in this in-class exercise, they are:

\- \*\*Hunan province administrative boundary layer at county level.\*\* This is a geospatial data set in ESRI shapefile format.

\- \*\*Hunan_2012.csv\*\*: This csv file contains selected Hunan's local development indicators in 2012.

## 2.3 Importing the data

In this section, we will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.

### 2.3.1 Import shapefile into R environment

The code chunk below uses [st_read()](https://r-spatial.github.io/sf/reference/st_read.html) of \*\*sf\*\* package to import Hunan shapefile into R. The imported shapefile will be \*\*simple features\*\* Object of \*\*sf\*\*.

```{r}
hunan <- st_read(dsn = "data/geospatial", 
                 layer = "Hunan")
```

### 2.3.2 Import csv file into R environment

Next, we will import *Hunan_2012.csv* into R by using *`read_csv()`* of **readr** package. The output is R data frame class.

```{r}
hunan2012 <- read_csv("data/aspatial/Hunan_2012.csv")
```

### 2.3.3 Performing relational join

The code chunk below will be used to update the attribute table of *hunan*'s SpatialPolygonsDataFrame with the attribute fields of *hunan2012* dataframe. This is performed by using *`left_join()`* of **dplyr** package.

```{r}
hunan_GDPPC <- left_join(hunan,hunan2012) %>%    
  select(1:4, 7, 15) # only pick out column 1-4, 7 and 15
#both have same observation, have the same unique identifier, so don't have to specify 
```

::: callout - important In order to retain the geospatial properties, the left data frame must be the sf data.frame (i.e hunan). :::

# 3. Plotting a chloropleth map

Now, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using *`qtm()`* of **tmap** package.

```{r}
basemap <- tm_shape(hunan_GDPPC) +
  tm_polygons() +
  tm_text("NAME_3", size=0.5)

gdppc <- qtm(hunan_GDPPC, "GDPPC")
tmap_arrange(basemap, gdppc, asp=1, ncol=2)
```

# 4. Deriving contiguity weights: Queen's method

In the code chunk below, queen method is used to derive the contiguity weights

```{r}
wm_q <- hunan_GDPPC %>%
  mutate(nb = st_contiguity(geometry),
         wt = st_weights(nb, 
                         style = "W"),
         .before = 1)

```

Notice that `st_weights()` provides three arguments, they are,

-   ***nb***: A neighbour list object as created by st_neighbours()

-   ***style***: Default "W" for row standardized weights. This value can be "B", "C", "U", "minmax", abd "S". B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbors (sum over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al.1999,p.167-168 (sum over all links to n)

-   ***allow_zero***: If TRUE, assigns zero as lagged value to zone without neighbours.

# 5. Cluster and Outlier Analysis

## 5.1 Computing local Moran's I

In this section, we will compute local Moran's I of GDPPC at county level by using local_moran() of sfdep package.

```{r}
lisa <- wm_q %>%
  mutate(local_moran =local_moran(
    GDPPC, nb, wt, nsim =99),
    .before = 1) %>%
  unnest(local_moran) #unnest to individual columns

```

use mean or median, look at the distribution, if GDPPC is skewed use median instead of mean.

The output of local_moran() is a sf data.frame containing the columns: **ii,eii,var_ii, z_ii,p_ii_sim**,and **p_folded_sim**.

```{r}
GDPPC <- read_csv("data/aspatial/Hunan_GDPPC.csv")
```

## 5.2 Creating a Time Series Cube

The code chunk below, `spacetime()` of sfdep is used to create a spacetime cube.

```{r}
GDPPC_st <- spacetime(GDPPC, hunan,
                      .loc_col = "County",
                      .time_col = "Year")
```

Next, `is_spacetime_cube()` of sfdep package will be used to verify if GDPPC_st is indeed a space_time cube object

```{r}
is_spacetime_cube(GDPPC_st)
```

The code chunk below will be used to identify neighbours and to derive an inverse distance weights.

```{r}
GDPPC_nb <- GDPPC_st %>%
  activate("geometry") %>%
  mutate(nb = include_self(st_contiguity(geometry)),
                           wt = st_inverse_distance(nb, geometry,
                                                    scale = 1,
                                                    alpha = 1),
                           .before = 1)%>%
           set_nbs("nb") %>%
           set_wts("wt")
```

### Compute Gi\*

We can use these two new columns to manually calculate the local Gi\* for each location. We can do this by grouping by Year and using local_gstar_perm() of sfdep package. After which, we use `unnest()` to unnest gi_star column of the newly created gi_stars data.frame.

```{r}
gi_stars <- GDPPC_nb %>%
  group_by(Year) %>%
  mutate(gi_star = local_gstar_perm(
    GDPPC, nb, wt)) %>%
  tidyr::unnest(gi_star)
```

### Mann-Kendall Test

With these Gi\* measures, we can evaluate each location for a trend using the Mann-Kendall test. The code chunk below uses Changsha county.

```{r}
cbg <- gi_stars %>%
  ungroup() %>%
filter(County == "Changsha") |>
  select(County, Year, gi_star)
  
```

Next, we plot the result by using the ggplot2 functions,

```{r}
p <- ggplot(data = cbg,
            aes(x = Year,
                y = gi_star)) +
  geom_line() + 
  theme_light()

ggplotly(p)
```

### Performing Emerging Hotspot Analysis

Lastly, we will perform EHSA analysis by using `emerging_hotspot_analysis()` of sfdep package. It takes a spacetime object c (i.e GDPPC_st), and the quoted name of the variable of interest (i.e. GDPPC) for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default. Lastly, nsim map numbers of simulation is performed.

```{r}
#ehsa <- emerging_hotspot_analysis(
#  x = GDPPC_st,
#  .var = "GDPPC",
#  k = 1,
#  nsim =99
#)
```

Next. tmap function will be used to plot a categorical choropleth map using the code chunk below.

```{r}
#ehsa_sig <- hunan_ehsa %>%
#  filter(p_value < 0.05)
#tmap_mode("plot")
#tm_shape(hunan_ehsa) + 
#  tm_polygons() +
#  tm_borders(alpha = 0.5) +
#  tm_shape(ehsa_sig) +
#  tm_fill("classification") +
#  tm_borders(alpha = 0.4)
```
